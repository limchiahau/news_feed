{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from abc import ABC\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site:\n",
    "    def __init__(self,name,posts):\n",
    "        self.name = name\n",
    "        self.posts = posts\n",
    "        \n",
    "    def __repr__(self):\n",
    "        #stub\n",
    "        return f'{self.name}\\n\\n\\n{self.posts_to_str()}'\n",
    "    \n",
    "    def filter_posts(self):\n",
    "        '''\n",
    "        remove unecessary posts from the posts attribute.\n",
    "        \n",
    "        to be implemented by a subclass.'''\n",
    "        pass\n",
    "    \n",
    "    def posts_to_str(self):\n",
    "        return '\\n\\n\\n'.join([f'{post}' for post in self.posts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post:\n",
    "    def __init__(self,title,content,link):\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        self.link = link\n",
    "    \n",
    "    def title_contains(self, target):\n",
    "        return target in self.title.lower()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        #stub\n",
    "        return f'TITLE:\\n\\n{self.title}\\n\\nCONTENT:\\n\\n{self.content}\\n\\nLINK:\\n\\n{self.link}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source():\n",
    "    def __init__(self,name,url):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        \n",
    "    def download(self) -> Site:\n",
    "        '''returns Site'''\n",
    "        source_html = requests.get(self.url).text\n",
    "        soup = BeautifulSoup(source_html,'html.parser')\n",
    "        \n",
    "        site = self.parse(soup)\n",
    "        site.filter_posts()\n",
    "        \n",
    "        return site\n",
    "    \n",
    "    def parse(self,soup) -> Site:\n",
    "        '''to be implemented by a subclass'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacrumorsSite(Site):\n",
    "    def filter_posts(self):\n",
    "        self.remove_giveaways_posts()\n",
    "        self.remove_deals()\n",
    "        \n",
    "    def remove_deals(self):\n",
    "        self.remove_posts(lambda post: post.title_contains('deals:'))\n",
    "        \n",
    "    def remove_giveaways_posts(self):\n",
    "        self.remove_posts(lambda post: post.title_contains('giveaway'))\n",
    "    \n",
    "    def remove_posts(self, filter_fn):\n",
    "        '''filter_fn is a function that returns true\n",
    "        if the post should be removed.'''\n",
    "        \n",
    "        self.posts = list(filter(lambda post: not filter_fn(post), self.posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacrumorsSource(Source):\n",
    "    def parse(self,soup) -> MacrumorsSite:\n",
    "        contents = self.parse_content(soup)\n",
    "        titles = self.parse_titles(soup)\n",
    "        links = self.parse_links(soup)\n",
    "        \n",
    "        posts_raw = zip(titles,contents,links)\n",
    "        posts = [Post(*post_data) for post_data in posts_raw]\n",
    "        \n",
    "        return MacrumorsSite(self.name, posts)\n",
    "    \n",
    "    def parse_content(self,soup):\n",
    "        content_list = soup.find_all(class_='js-contentInner')\n",
    "        content_list = [self.cleanup_content(content.get_text()) for content in content_list]\n",
    "        return content_list\n",
    "    \n",
    "    def parse_titles(self,soup):\n",
    "        return self.get_all_titles(lambda title: title.get_text(), soup)\n",
    "    \n",
    "    def parse_links(self,soup):\n",
    "        return self.get_all_titles(lambda title: title.find('a')['href'],soup)\n",
    "    \n",
    "    def get_all_titles(self,fn,soup):\n",
    "        '''returns [fn(title)]'''\n",
    "        articles = soup.find_all(class_='js-article')\n",
    "        titles = map(self.get_title, articles)\n",
    "        return [fn(title) for title in titles]\n",
    "    \n",
    "    def get_title(self,article):\n",
    "        '''\n",
    "        gets the first child of the article.\n",
    "        \n",
    "        This is more resilient than looking for certain tags.\n",
    "        '''\n",
    "        return next(article.children)\n",
    "        \n",
    "    def cleanup_content(self,content):\n",
    "        # corrects the content text.\n",
    "        replacements = [\n",
    "            ('\\r ','\\n'*2), #replace linebreaks\n",
    "            ('  ', ' '), #fix double spaces\n",
    "            ('\\n','\\n\\n')\n",
    "        ]\n",
    "        for r in replacements:\n",
    "            content = content.replace(*r)\n",
    "            \n",
    "        # removes repertitive whitespace\n",
    "        content = re.sub(r'\\n\\n+[\\n]','\\n\\n',content)\n",
    "\n",
    "        # remove errors in the content text\n",
    "        errors = [\n",
    "            '\\u200c',\n",
    "            'img.lazyload { display: none; }',\n",
    "            'img.lazyload { display: none; } '\n",
    "        ]\n",
    "        \n",
    "        for e in errors:\n",
    "            # removes error\n",
    "            content = content.replace(e, '')\n",
    "            \n",
    "        # remove leading blankspace\n",
    "        paragraphs = content.split('\\n'*2)\n",
    "        paragraphs = [p.strip() for p in paragraphs]\n",
    "        content = ('\\n'*2).join(paragraphs)\n",
    "        \n",
    "        # remove trailing blankspace\n",
    "        content = content.strip()\n",
    "            \n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevent_exit():\n",
    "    input('Press enter to exit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MacrumorsSource('Macrumors','https://www.macrumors.com/')\n",
    "site = m.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(site)\n",
    "prevent_exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
